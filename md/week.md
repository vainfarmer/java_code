{"sessionId":16903,"dataScope":{"tableUidList":[],"chatBITopicList":[],"chatDatasetInfoList":[],"assetsList":[],"dataMart":[]},"question":"What's the order-level GMV for the last 7 days?","richText":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What's the order-level GMV for the last 7 days?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"start\",\"indent\":0,\"type\":\"paragraph\",\"version\":1,\"textFormat\":0,\"textStyle\":\"\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","extendContext":"","askAgain":false,"commonInfo":{"user":"zhilong.zhang","region":"REG","userEmail":"zhilong.zhang@shopee.com"}}

https://datasuite.staging.shopee.io/assistant/open/commonchat/chat/stream/[stress]()



{"type":"CommonChat","sessionName":"What's the order-level GMV for the last 7 days?","sessionScope":{"tableUidList":[],"chatBITopicList":[],"chatDatasetInfoList":[],"assetsList":[],"dataMart":[]},"commonInfo":{"user":"zhilong.zhang","region":"REG","userEmail":"zhilong.zhang@shopee.com"}}



{"sessionId":18264,"dataScope":{"tableUidList":[],"chatBITopicList":[],"chatDatasetInfoList":[],"assetsList":[],"dataMart":[]},"question":"What's the order-level GMV for the last 7 days?","richText":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What's the order-level GMV for the last 7 days?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"start\",\"indent\":0,\"type\":\"paragraph\",\"version\":1,\"textFormat\":0,\"textStyle\":\"\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","extendContext":"","askAgain":false,"commonInfo":{"user":"zhilong.zhang","region":"REG","userEmail":"zhilong.zhang@shopee.com"}}



{"type":"CommonChat","sessionName":"how to create a csv2hive task by DataHub?","sessionScope":{"tableUidList":[],"chatBITopicList":[],"chatDatasetInfoList":[],"assetsList":[],"dataMart":[]},"commonInfo":{"user":"zhilong.zhang","region":"REG","userEmail":"zhilong.zhang@shopee.com"}}

https://datasuite.staging.shopee.io/assistant/session/new

{"sessionId":18202,"dataScope":{"tableUidList":[],"chatBITopicList":[],"chatDatasetInfoList":[],"assetsList":[],"dataMart":[]},"question":"how to create a csv2hive task by DataHub","richText":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"how to create a csv2hive task by DataHub\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"start\",\"indent\":0,\"type\":\"paragraph\",\"version\":1,\"textFormat\":0,\"textStyle\":\"\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","extendContext":"","askAgain":false,"commonInfo":{"user":"zhilong.zhang","region":"REG","userEmail":"zhilong.zhang@shopee.com"}}



```
#!/usr/bin/env python3
"""
Simple SSE stress tester for the Datasuite assistant stream endpoint.

Features:
- Configurable concurrency and total connection count (hardcoded defaults).
- Uses hardcoded default URL / payload (no file dependency).
- Prints every SSE event along with timestamps and extracted trace ids.
- Records per-connection metrics for quick post-run inspection.
"""

import argparse
import asyncio
import json
import signal
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional
import copy

import aiohttp

DEFAULT_URL = (
    "https://datasuite.staging.shopee.io/assistant/open/commonchat/chat/stream/stress"
)

DEFAULT_PAYLOAD: Dict[str, Any] = {
    "type": "CommonChat",
    "sessionName": "æ¯å¤©æœ‰å¤šå°‘ä¼šè¯æ•°test",
    "sessionScope": {
        "tableUidList": [],
        "chatBITopicList": [],
        "chatDatasetInfoList": [],
        "assetsList": [],
        "dataMart": [],
    },

    "dataScope": {
        "tableUidList": [
            "SG.data_infra.shopee_di_rag_db__chat_message_tab__reg_continuous_s0_live"
        ],
        "chatBITopicList": [],
        "chatDatasetInfoList": [],
        "assetsList": [],
        "dataMart": [],
    },
    "question": "æ¯å¤©æœ‰å¤šå°‘ä¼šè¯æ•°",
    "richText": '{"root":{"children":[{"children":[{"detail":0,"format":0,"mode":"normal","style":"","text":"æ¯å¤©æœ‰å¤šå°‘ä¼šè¯æ•°","type":"text","version":1}],"direction":"ltr","format":"","indent":0,"type":"paragraph","version":1,"textFormat":0,"textStyle":""}],"direction":"ltr","format":"","indent":0,"type":"root","version":1}}',
    "askAgain": False,
    "extendContext": "",
    "commonInfo": {
        "user": "yiming.feng",
        "region": "REG",
        "userEmail": "yiming.feng@shopee.com",
    },
}
DEFAULT_LOG_FILE = Path(
    f"/Users/zhilong.zhang/PycharmProjects/PythonProject/code/stress_test/stress_{time.strftime('%Y%m%d_%H%M%S')}.log"
)
DEFAULT_REQUESTS = 20
DEFAULT_CONCURRENCY = 50
DEFAULT_MAX_EVENTS = -1
DEFAULT_SOCK_CONNECT_TIMEOUT = 60.0
DEFAULT_HEADER_ITEMS: List[str] = []


def make_logger(log_path: Path):
    """Create a logger that prints to stdout and appends to a log file."""
    log_path.parent.mkdir(parents=True, exist_ok=True)

    def _log(message: str) -> None:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        line = f"[{timestamp}] {message}"
        print(line)
        try:
            with log_path.open("a", encoding="utf-8") as f:
                f.write(line + "\n")
        except Exception:
            # å¦‚æœå†™æ—¥å¿—å¤±è´¥ï¼Œè‡³å°‘ä¿è¯æ§åˆ¶å°è¾“å‡º
            pass

    return _log


def load_payload(args: argparse.Namespace) -> Dict[str, Any]:
    if args.payload_inline:
        return json.loads(args.payload_inline)
    if args.payload_file:
        return json.loads(Path(args.payload_file).read_text(encoding="utf-8"))
    # fallback to hardcoded default
    return DEFAULT_PAYLOAD


def resolve_url(args: argparse.Namespace) -> str:
    return DEFAULT_URL


def parse_headers(header_items: List[str]) -> Dict[str, str]:
    headers: Dict[str, str] = {}
    for item in header_items:
        if ":" not in item:
            raise ValueError(f"Header éœ€è¦ä½¿ç”¨ key:value æ ¼å¼, å½“å‰: {item}")
        key, value = item.split(":", 1)
        headers[key.strip()] = value.strip()
    return headers


@dataclass
class SSERecord:
    conn_id: int
    start_ts: float
    end_ts: Optional[float] = None
    event_count: int = 0
    trace_ids: set[str] = field(default_factory=set)
    error: Optional[str] = None

    def finish(self) -> None:
        self.end_ts = time.time()

    def to_summary(self) -> Dict[str, Any]:
        return {
            "conn_id": self.conn_id,
            "start": self.start_ts,
            "end": self.end_ts,
            "cost_time": (self.end_ts - self.start_ts) if self.end_ts else None,
            "events": self.event_count,
            "trace_ids": list(self.trace_ids),
            "error": self.error,
        }


def extract_trace(event_payload: str) -> Optional[str]:
    try:
        parsed = json.loads(event_payload)
    except json.JSONDecodeError:
        return None
    for key in ("trace_id", "traceId", "traceID", "trace"):
        if isinstance(parsed, dict) and key in parsed:
            return str(parsed[key])
    return None


async def consume_sse(
    conn_id: int,
    session: aiohttp.ClientSession,
    args: argparse.Namespace,
    payload: Dict[str, Any],
    semaphore: asyncio.Semaphore,
    log,
) -> SSERecord:
    record = SSERecord(conn_id=conn_id, start_ts=time.time())
    async with semaphore:
        sid = payload.get("sessionId")
        log(
            f"[Conn {conn_id}] å¼€å§‹ {time.strftime('%H:%M:%S', time.localtime(record.start_ts))} | sessionId={sid}"
        )
        try:
            async with session.post(
                args.url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=None, sock_read=None),
                headers=args.headers,
            ) as resp:
                resp.raise_for_status()
                buffer = ""
                async for chunk in resp.content.iter_chunked(1024 * 1024):
                    if not chunk:
                        break
                    decoded = chunk.decode("utf-8", errors="ignore")
                    buffer += decoded
                    print(buffer)
                    # æŒ‰ SSE è§„èŒƒï¼Œäº‹ä»¶ä¹‹é—´ä»¥ç©ºè¡Œåˆ†éš”
                    while "\n\n" in buffer:
                        event, buffer = buffer.split("\n\n", 1)
                        lines = event.splitlines()
                        data_lines: List[str] = []
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            if line.startswith("data:"):
                                data_lines.append(line[5:].strip())
                            else:
                                log(f"[Conn {conn_id}] meta: {line}")
                        if data_lines:
                            data = "\n".join(data_lines)
                            record.event_count += 1
                            trace_id = extract_trace(data)
                            if trace_id:
                                record.trace_ids.add(trace_id)
                            log(f"[Conn {conn_id}] event #{record.event_count}: {data}")
                            if "Something went wrong" in data:
                                log(
                                    f"[Conn {conn_id}] ERROR_EVENT: payload={payload} | event={data}"
                                )
                            if 0 < args.max_events <= record.event_count:
                                break
        except Exception as exc:  # pylint: disable=broad-except
            record.error = repr(exc)
            log(f"[Conn {conn_id}] å‘ç”Ÿå¼‚å¸¸: {exc}")
        finally:
            record.finish()
            log(
                f"[Conn {conn_id}] ç»“æŸ {time.strftime('%H:%M:%S', time.localtime(record.end_ts))} "
                f"è€—æ—¶ {record.end_ts - record.start_ts:.2f}s | events={record.event_count}"
            )
    return record


async def runner(args: argparse.Namespace, log) -> None:
    payload_template = load_payload(args)
    args.url = resolve_url(args)
    args.headers = parse_headers(DEFAULT_HEADER_ITEMS)
    args.requests = DEFAULT_REQUESTS
    args.concurrency = min(DEFAULT_CONCURRENCY, args.requests)
    args.max_events = DEFAULT_MAX_EVENTS
    args.sock_connect_timeout = DEFAULT_SOCK_CONNECT_TIMEOUT

    semaphore = asyncio.Semaphore(args.concurrency)
    timeout = aiohttp.ClientTimeout(
        total=None, sock_connect=args.sock_connect_timeout, sock_read=None
    )

    async with aiohttp.ClientSession(timeout=timeout) as session:
        tasks = [
            asyncio.create_task(
                consume_sse(
                    i + 1,
                    session,
                    args,
                    copy.deepcopy(payload_template),
                    semaphore,
                    log,
                )
            )
            for i in range(args.requests)
        ]
        results = await asyncio.gather(*tasks)

    log("\n==== æ±‡æ€» ====")
    for item in results:
        log(str(item.to_summary()))


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="SSE å‹æµ‹å·¥å…·")
    parser.add_argument("--payload-file", type=str, help="ç›´æ¥è¯»å– JSON payload çš„æ–‡ä»¶")
    parser.add_argument("--payload-inline", type=str, help="ç›´æ¥ä¼ å…¥ JSON å­—ç¬¦ä¸²")
    parser.add_argument(
        "--log-file",
        type=Path,
        default=DEFAULT_LOG_FILE,
        help="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤å†™å…¥ /Users/zhilong.zhang/PycharmProjects/PythonProject/code/stress_test/stress.log",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    logger = make_logger(args.log_file)

    loop = asyncio.get_event_loop()

    def handle_stop(signame: str) -> None:
        logger(f"æ”¶åˆ° {signame}ï¼Œå‡†å¤‡é€€å‡º...")
        for task in asyncio.all_tasks(loop):
            task.cancel()

    for signame in ("SIGINT", "SIGTERM"):
        if hasattr(signal, signame):
            loop.add_signal_handler(
                getattr(signal, signame), lambda s=signame: handle_stop(s)
            )

    try:
        loop.run_until_complete(runner(args, logger))
    except asyncio.CancelledError:
        logger("ä»»åŠ¡è¢«å–æ¶ˆ")
    finally:
        loop.close()


if __name__ == "__main__":
    main()
```





```
cd /usr/local/redis-7.0.15
redis-server
redis-cli shutdown
ps aux | grep redis-server
sudo kill -9 

brew services start mysql
brew services stop mysql


cd /usr/local/etc
bin/zookeeper-server-start.sh config/zookeeper.properties 
bin/kafka-server-start.sh config/server.properties
# åˆ›å»ºtopic
bin/kafka-topics.sh --create --topic test-events --bootstrap-server localhost:9092
# æŸ¥çœ‹çŠ¶æ€
bin/kafka-topics.sh --describe --topic flash-sale-order --bootstrap-server localhost:9092
# å‘é€æ¶ˆæ¯
bin/kafka-console-producer.sh --topic test-events --bootstrap-server localhost:9092
Hello, Kafka
This is my first enent


qps 1500

```



```
============================================================
ğŸ“Š å‹æµ‹ç»“æœç»Ÿè®¡ [ç›´æ¥æ•°æ®åº“ï¼ˆå¼‚æ­¥/è™šæ‹Ÿçº¿ç¨‹ï¼‰]
============================================================

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: 100
   æˆåŠŸæ•°é‡: 70 (70.0%)
   å¤±è´¥æ•°é‡: 30 (30.0%)
   æ€»è€—æ—¶: 0.71ç§’
   QPS: 141.72

â±ï¸ å“åº”æ—¶é—´(ms):
   å¹³å‡: 572.89
   æœ€å°: 339.16
   æœ€å¤§: 703.36
   P50: 598.11
   P95: 698.14
   P99: 703.36

âŒ å¤±è´¥åŸå› åˆ†å¸ƒ:
   ç³»ç»Ÿç¹å¿™ï¼Œè¯·é‡è¯•: 30æ¬¡

âœ… æˆåŠŸæŠ¢è´­ç”¨æˆ·: 70äºº
   ç”¨æˆ·2: FS2005839390672302080
   ç”¨æˆ·3: FS2005839389455953920
   ç”¨æˆ·4: FS2005839389728583680
   ç”¨æˆ·6: FS2005839390802325504
   ç”¨æˆ·7: FS2005839389518868480
   ... å…±70äºº

============================================================





============================================================
ğŸ“Š å‹æµ‹ç»“æœç»Ÿè®¡ [ç›´æ¥æ•°æ®åº“ï¼ˆåŒæ­¥ï¼‰]
============================================================

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: 100
   æˆåŠŸæ•°é‡: 4 (4.0%)
   å¤±è´¥æ•°é‡: 96 (96.0%)
   æ€»è€—æ—¶: 1.51ç§’
   QPS: 66.41

â±ï¸ å“åº”æ—¶é—´(ms):
   å¹³å‡: 797.43
   æœ€å°: 129.62
   æœ€å¤§: 1504.02
   P50: 677.94
   P95: 1456.84
   P99: 1504.02

âŒ å¤±è´¥åŸå› åˆ†å¸ƒ:
   ç³»ç»Ÿç¹å¿™ï¼Œè¯·é‡è¯•: 96æ¬¡

âœ… æˆåŠŸæŠ¢è´­ç”¨æˆ·: 4äºº
   ç”¨æˆ·57: FS2005841893979791360
   ç”¨æˆ·64: FS2005841896274075648
   ç”¨æˆ·78: FS2005841892197212160
   ç”¨æˆ·83: FS2005841892989935616

============================================================


============================================================
ğŸ“Š å‹æµ‹ç»“æœç»Ÿè®¡ [ç›´æ¥æ•°æ®åº“ï¼ˆå¼‚æ­¥/è™šæ‹Ÿçº¿ç¨‹ï¼‰]
============================================================

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: 100
   æˆåŠŸæ•°é‡: 25 (25.0%)
   å¤±è´¥æ•°é‡: 75 (75.0%)
   æ€»è€—æ—¶: 0.44ç§’
   QPS: 229.58

â±ï¸ å“åº”æ—¶é—´(ms):
   å¹³å‡: 388.37
   æœ€å°: 159.03
   æœ€å¤§: 433.47
   P50: 414.54
   P95: 432.20
   P99: 433.47

âŒ å¤±è´¥åŸå› åˆ†å¸ƒ:
   ç³»ç»Ÿç¹å¿™ï¼Œè¯·é‡è¯•: 72æ¬¡
   è¶…å‡ºé™è´­æ•°é‡: 3æ¬¡

âœ… æˆåŠŸæŠ¢è´­ç”¨æˆ·: 25äºº
   ç”¨æˆ·4: FS2005844293796913152
   ç”¨æˆ·6: FS2005844294270869504
   ç”¨æˆ·9: FS2005844294451224576
   ç”¨æˆ·14: FS2005844294300229632
   ç”¨æˆ·18: FS2005844293704638464
   ... å…±25äºº

============================================================


============================================================
ğŸ“Š å‹æµ‹ç»“æœç»Ÿè®¡ [Redis + Lua + Kafkaï¼ˆå¼‚æ­¥/è™šæ‹Ÿçº¿ç¨‹ï¼‰]
============================================================

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: 3000
   æˆåŠŸæ•°é‡: 3000 (100.0%)
   å¤±è´¥æ•°é‡: 0 (0.0%)
   æ€»è€—æ—¶: 1.98ç§’
   QPS: 1514.60

â±ï¸ å“åº”æ—¶é—´(ms):
   å¹³å‡: 1665.17
   æœ€å°: 1074.95
   æœ€å¤§: 1959.07
   P50: 1728.74
   P95: 1903.43
   P99: 1943.60

âœ… æˆåŠŸæŠ¢è´­ç”¨æˆ·: 3000äºº
   ç”¨æˆ·1: FS2005845077880102916
   ç”¨æˆ·2: FS2005845075665510405
   ç”¨æˆ·3: FS2005845078370836487
   ç”¨æˆ·4: FS2005845077741690884
   ç”¨æˆ·5: FS2005845077930434562
   ... å…±3000äºº

============================================================



============================================================
ğŸ“Š å‹æµ‹ç»“æœç»Ÿè®¡ [Redis + Lua + Kafkaï¼ˆåŒæ­¥ï¼‰]
============================================================

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: 3000
   æˆåŠŸæ•°é‡: 3000 (100.0%)
   å¤±è´¥æ•°é‡: 0 (0.0%)
   æ€»è€—æ—¶: 1.64ç§’
   QPS: 1826.65

â±ï¸ å“åº”æ—¶é—´(ms):
   å¹³å‡: 1441.71
   æœ€å°: 1059.11
   æœ€å¤§: 1634.63
   P50: 1475.00
   P95: 1582.34
   P99: 1627.13

âœ… æˆåŠŸæŠ¢è´­ç”¨æˆ·: 3000äºº
   ç”¨æˆ·1: FS2005845373393985544
   ç”¨æˆ·2: FS2005845373427539996
   ç”¨æˆ·3: FS2005845373565952003
   ç”¨æˆ·4: FS2005845373524008984
   ç”¨æˆ·5: FS2005845373452705815
   ... å…±3000äºº

============================================================
```

